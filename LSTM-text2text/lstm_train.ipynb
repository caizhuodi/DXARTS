{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99b5ec1-e57f-4b5c-9090-546bc7b6b249",
   "metadata": {},
   "source": [
    "This tutorial is based on https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/#comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676ecd34-efb3-4426-b9b7-93f7205aded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 07:49:22.482753: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-02 07:49:22.482810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-02 07:49:22.484160: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-02 07:49:22.492215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd96679-ee5e-4eff-ad5e-af3f6cb63137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a9770b-dad5-47a0-b3b0-01726cc2f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../Zoe2.0_Transformer/medium_articles.csv\")\n",
    "# df = df[[\"text\"]]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7f1639-19f5-4645-b0a3-43f8dcfaeccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c699b386-bdac-4316-9095-d7ac553f602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2637884f-79f3-4d51-b358-9253ad27ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text = df['text'].iloc[:999].str.cat(sep='. ')\n",
    "# raw_text[:10]\n",
    "# len(raw_text)  # full text length is 981028347\n",
    "\n",
    "# with open(\"medium_articles.txt\", \"w\") as file:\n",
    "#     file.write(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f593c6e-8518-4aef-b5bc-b057bac76e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"anna.txt\"\n",
    "# filename = \"medium_articles.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text = raw_text.replace(\"\\n\", \" \")\n",
    "# raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73364048-6e40-4365-9484-8b958bfc11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bec4d06-a744-4ed6-a9e1-3631ac768da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1985223\n",
      "Total Vocab:  56\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72729902-06c5-4556-8eab-de33baa7dcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 50\n",
    "n_chars - seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ffffaa-583c-4bf4-ae54-0dae64484b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985123"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "n_chars - seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66edacc9-fb96-41e1-871f-56580793a7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"chapter 1   happy families are all alike; every unhappy family is unhappy in its own way.  everything was in confusion in the oblonskys' house. the wife had discovered that the husband was carrying on an intrigue with a french girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. this position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. every person in the house felt that there was no sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the members of the family and household of the oblonskys. the wife did not leave her own room, the husband had not been at home for three days. the children ran wild all over the house; the english governess quarreled with the housekeeper, and wrote to a \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e09389c-56ff-42f3-967a-14f875c13fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "276d6934-e90d-41ce-b54f-914586b98c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1985123/1985123 [00:26<00:00, 74558.69iteration/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1985123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# for i in range(0, n_chars - seq_length, 1):\n",
    "for i in tqdm(range(0, n_chars - seq_length, 1), desc=\"Processing\", unit=\"iteration\"):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3aa330-bcf7-47ce-967e-434757cdf903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "\n",
    "print(y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "802f806c-9ea0-4a14-9756-a246a1c21599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 07:50:03.367206: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.368780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.402147: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.403694: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.405249: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.406705: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.708772: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.710556: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.712096: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.713560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.714993: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.716432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.733947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.735423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.736944: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.738407: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.739853: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.741270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 77794 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:06:00.0, compute capability: 8.0\n",
      "2024-01-02 07:50:03.741933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-02 07:50:03.743339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78666 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:07:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "\n",
    "# small size model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# larger size model\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5825274d-e341-4ae2-a184-a062033da8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoints\n",
    "filepath=\"./epoch_new_1_anna/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e14f7762-9eaf-4d7b-9036-27a9db97408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 07:50:13.671190: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "2024-01-02 07:50:14.598926: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc87240cf80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-02 07:50:14.598963: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-01-02 07:50:14.598973: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-01-02 07:50:14.605853: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704181814.748753 1255888 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15509/15509 [==============================] - ETA: 0s - loss: 2.0692\n",
      "Epoch 1: loss improved from inf to 2.06918, saving model to ./epoch_new_1_anna/weights-improvement-01-2.0692-bigger.hdf5\n",
      "15509/15509 [==============================] - 260s 16ms/step - loss: 2.0692\n",
      "Epoch 2/20\n",
      "    4/15509 [..............................] - ETA: 4:38 - loss: 1.7344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15507/15509 [============================>.] - ETA: 0s - loss: 1.6211\n",
      "Epoch 2: loss improved from 2.06918 to 1.62113, saving model to ./epoch_new_1_anna/weights-improvement-02-1.6211-bigger.hdf5\n",
      "15509/15509 [==============================] - 246s 16ms/step - loss: 1.6211\n",
      "Epoch 3/20\n",
      "15508/15509 [============================>.] - ETA: 0s - loss: 1.4901\n",
      "Epoch 3: loss improved from 1.62113 to 1.49009, saving model to ./epoch_new_1_anna/weights-improvement-03-1.4901-bigger.hdf5\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.4901\n",
      "Epoch 4/20\n",
      "15507/15509 [============================>.] - ETA: 0s - loss: 1.4183\n",
      "Epoch 4: loss improved from 1.49009 to 1.41827, saving model to ./epoch_new_1_anna/weights-improvement-04-1.4183-bigger.hdf5\n",
      "15509/15509 [==============================] - 246s 16ms/step - loss: 1.4183\n",
      "Epoch 5/20\n",
      "15509/15509 [==============================] - ETA: 0s - loss: 1.3727\n",
      "Epoch 5: loss improved from 1.41827 to 1.37266, saving model to ./epoch_new_1_anna/weights-improvement-05-1.3727-bigger.hdf5\n",
      "15509/15509 [==============================] - 244s 16ms/step - loss: 1.3727\n",
      "Epoch 6/20\n",
      "15506/15509 [============================>.] - ETA: 0s - loss: 1.3394\n",
      "Epoch 6: loss improved from 1.37266 to 1.33943, saving model to ./epoch_new_1_anna/weights-improvement-06-1.3394-bigger.hdf5\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.3394\n",
      "Epoch 7/20\n",
      "15508/15509 [============================>.] - ETA: 0s - loss: 1.3147\n",
      "Epoch 7: loss improved from 1.33943 to 1.31468, saving model to ./epoch_new_1_anna/weights-improvement-07-1.3147-bigger.hdf5\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.3147\n",
      "Epoch 8/20\n",
      "15509/15509 [==============================] - ETA: 0s - loss: 1.2963\n",
      "Epoch 8: loss improved from 1.31468 to 1.29627, saving model to ./epoch_new_1_anna/weights-improvement-08-1.2963-bigger.hdf5\n",
      "15509/15509 [==============================] - 250s 16ms/step - loss: 1.2963\n",
      "Epoch 9/20\n",
      "15506/15509 [============================>.] - ETA: 0s - loss: 1.2805\n",
      "Epoch 9: loss improved from 1.29627 to 1.28054, saving model to ./epoch_new_1_anna/weights-improvement-09-1.2805-bigger.hdf5\n",
      "15509/15509 [==============================] - 243s 16ms/step - loss: 1.2805\n",
      "Epoch 10/20\n",
      "15506/15509 [============================>.] - ETA: 0s - loss: 1.2681\n",
      "Epoch 10: loss improved from 1.28054 to 1.26811, saving model to ./epoch_new_1_anna/weights-improvement-10-1.2681-bigger.hdf5\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.2681\n",
      "Epoch 11/20\n",
      "15509/15509 [==============================] - ETA: 0s - loss: 1.2585\n",
      "Epoch 11: loss improved from 1.26811 to 1.25852, saving model to ./epoch_new_1_anna/weights-improvement-11-1.2585-bigger.hdf5\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.2585\n",
      "Epoch 12/20\n",
      "15509/15509 [==============================] - ETA: 0s - loss: 1.2508\n",
      "Epoch 12: loss improved from 1.25852 to 1.25082, saving model to ./epoch_new_1_anna/weights-improvement-12-1.2508-bigger.hdf5\n",
      "15509/15509 [==============================] - 248s 16ms/step - loss: 1.2508\n",
      "Epoch 13/20\n",
      "15507/15509 [============================>.] - ETA: 0s - loss: 1.2426\n",
      "Epoch 13: loss improved from 1.25082 to 1.24266, saving model to ./epoch_new_1_anna/weights-improvement-13-1.2427-bigger.hdf5\n",
      "15509/15509 [==============================] - 252s 16ms/step - loss: 1.2427\n",
      "Epoch 14/20\n",
      "15509/15509 [==============================] - ETA: 0s - loss: 1.2366\n",
      "Epoch 14: loss improved from 1.24266 to 1.23656, saving model to ./epoch_new_1_anna/weights-improvement-14-1.2366-bigger.hdf5\n",
      "15509/15509 [==============================] - 256s 16ms/step - loss: 1.2366\n",
      "Epoch 15/20\n",
      "15508/15509 [============================>.] - ETA: 0s - loss: 1.2310\n",
      "Epoch 15: loss improved from 1.23656 to 1.23096, saving model to ./epoch_new_1_anna/weights-improvement-15-1.2310-bigger.hdf5\n",
      "15509/15509 [==============================] - 262s 17ms/step - loss: 1.2310\n",
      "Epoch 16/20\n",
      "15507/15509 [============================>.] - ETA: 0s - loss: 1.2248\n",
      "Epoch 16: loss improved from 1.23096 to 1.22479, saving model to ./epoch_new_1_anna/weights-improvement-16-1.2248-bigger.hdf5\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.2248\n",
      "Epoch 17/20\n",
      "15506/15509 [============================>.] - ETA: 0s - loss: 1.2211\n",
      "Epoch 17: loss improved from 1.22479 to 1.22115, saving model to ./epoch_new_1_anna/weights-improvement-17-1.2212-bigger.hdf5\n",
      "15509/15509 [==============================] - 246s 16ms/step - loss: 1.2212\n",
      "Epoch 18/20\n",
      "15508/15509 [============================>.] - ETA: 0s - loss: 1.2191\n",
      "Epoch 18: loss improved from 1.22115 to 1.21912, saving model to ./epoch_new_1_anna/weights-improvement-18-1.2191-bigger.hdf5\n",
      "15509/15509 [==============================] - 246s 16ms/step - loss: 1.2191\n",
      "Epoch 19/20\n",
      "15506/15509 [============================>.] - ETA: 0s - loss: 1.2158\n",
      "Epoch 19: loss improved from 1.21912 to 1.21584, saving model to ./epoch_new_1_anna/weights-improvement-19-1.2158-bigger.hdf5\n",
      "15509/15509 [==============================] - 246s 16ms/step - loss: 1.2158\n",
      "Epoch 20/20\n",
      "15507/15509 [============================>.] - ETA: 0s - loss: 1.2406\n",
      "Epoch 20: loss did not improve from 1.21584\n",
      "15509/15509 [==============================] - 245s 16ms/step - loss: 1.2405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fca6c703510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a7236-f287-49f3-9a68-a6516bd19b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
