{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107eaa10-61cc-48ba-bf12-daa696280d9e",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "This tutorial is based on https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7fa4d-c124-43bc-91cb-46ed8200e537",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "## 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e93ea54-b2d3-4c0a-9f7b-1ae46c19444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 00:24:10.788771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-31 00:24:10.788829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-31 00:24:10.790088: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-31 00:24:10.798020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1e8a6-2459-47e4-bbd3-8e97acd44895",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "### 1.1 read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91622cb3-16a3-4695-809c-e46e129210c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "A Pattern Language is the second in a series of books which describe an entirely new attitude to architecture and planning. The books are intended to provide a complete working alternative to our present ideas about architecture, building, and planni\n",
      "Length of text: 1373665 characters\n"
     ]
    }
   ],
   "source": [
    "file = open(\"../Zoe2.0_Markov/pattern_fix2.txt\", \"r\")  # open our txt file, read only (which means we can't use \"write()\" function later)\n",
    "text = file.read()\n",
    "\n",
    "print(type(text))  # our data comes in as string\n",
    "print(text[:250])  # print 0~100 elements of the string\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b529f433-be67-4261-8def-86743f105292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0927a-5c70-41be-8f33-dfd524c21c96",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "### 1.2 perform text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e66f2-a63f-41c8-a8be-c755c9b43d63",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "#### warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ddd5e49-d829-4eb8-abc6-3ff695c564c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 00:24:16.428630: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.431018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.469060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.471373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.473623: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.475861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.767264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.768871: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.770390: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.771855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.773301: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.774734: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.792516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.794002: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.795496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.797136: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.798577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.799994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79086 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:06:00.0, compute capability: 8.0\n",
      "2023-12-31 00:24:16.800465: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-31 00:24:16.801869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79086 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:07:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418baf1d-b27f-4813-b273-f491016a7461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.preprocessing.string_lookup.StringLookup at 0x7fc031455590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "ids_from_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe3a214-10f0-474d-872f-837dc29badef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[63, 64, 65, 66, 67, 68, 69], [86, 87, 88]]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04067be8-1405-47b5-b80c-e9e1895e5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup( vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f144018c-77a6-4aa2-8a5c-719deed362f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0e669c-ed3b-4027-8ec6-bb00f132127e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94417933-5b81-417d-a709-ca5c38f9aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b783e4-a0b3-43bd-976d-b21df65a7b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'abcdefg', b'xyz'], dtype=object)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_from_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d429d8b-acb3-40ad-bbf7-e270e3fc3af2",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "#### start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744eb771-4362-4f7a-a4a5-ac02b35b7e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1373665,), dtype=int64, numpy=array([33,  2, 48, ..., 87, 16,  1])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682b87e6-662a-472c-ba14-265583fc65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "201a5cbb-1101-43c9-8b40-c7e052b3dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      " \n",
      "P\n",
      "a\n",
      "t\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc65d55-5f81-458e-ad87-75755e8f6e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'A' b' ' b'P' b'a' b't' b't' b'e' b'r' b'n' b' ' b'L' b'a' b'n' b'g'\n",
      " b'u' b'a' b'g' b'e' b' ' b'i' b's' b' ' b't' b'h' b'e' b' ' b's' b'e'\n",
      " b'c' b'o' b'n' b'd' b' ' b'i' b'n' b' ' b'a' b' ' b's' b'e' b'r' b'i'\n",
      " b'e' b's' b' ' b'o' b'f' b' ' b'b' b'o' b'o' b'k' b's' b' ' b'w' b'h'\n",
      " b'i' b'c' b'h' b' ' b'd' b'e' b's' b'c' b'r' b'i' b'b' b'e' b' ' b'a'\n",
      " b'n' b' ' b'e' b'n' b't' b'i' b'r' b'e' b'l' b'y' b' ' b'n' b'e' b'w'\n",
      " b' ' b'a' b't' b't' b'i' b't' b'u' b'd' b'e' b' ' b't' b'o' b' ' b'a'\n",
      " b'r' b'c' b'h'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b38efe-9a4f-4c41-baeb-b5cc6efa3b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A Pattern Language is the second in a series of books which describe an entirely new attitude to arch'\n",
      "b'itecture and planning. The books are intended to provide a complete working alternative to our presen'\n",
      "b't ideas about architecture, building, and planning \\xe2\\x80\\x94 an alternative which will, we hope, gradually re'\n",
      "b'place current ideas and practices.\\nVolume 1, The Timeless Way of Building, and Volume 2, A Pattern La'\n",
      "b'nguage, are two halves of a single work. This book provides a language, for building and planning; th'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af026efe-0bda-469c-8df5-796616d881d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db67746-fa8f-452f-a146-bc8a052535ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad922b6-fa67-41e7-8096-b24986d8c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1400f73-8850-4695-ad50-d30f5d2b730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'A Pattern Language is the second in a series of books which describe an entirely new attitude to arc'\n",
      "Target: b' Pattern Language is the second in a series of books which describe an entirely new attitude to arch'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76251642-eaf2-4d8a-a49b-3410b66e3c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7dba9fb-fb0b-4fe4-861e-61f163f154b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "472130c7-7d56-4af8-88f6-aab34587e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67857c7e-9c82-4e90-b9af-3e7e582d2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d58b036-eb1c-4e65-820b-4f9790e48415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 108) # (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 03:39:57.112520: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f9fe708-9bde-4f02-bc31-1a666613734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  27648     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  110700    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4076652 (15.55 MB)\n",
      "Trainable params: 4076652 (15.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64c35ee9-6fb0-4b17-9dff-7be2dc52f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "659f085c-420c-4508-b726-cc512f06e79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13,   3,  63,   5,  35,  46,  79,  26,  68,  79,  96,  96,  83,\n",
       "        75, 101,  91,   1,  85,  38,  89,  88,  86,  58,  33,  33,  33,\n",
       "        82,  12,   8,  55,   8,  19, 101,  99,  48,  89,  37,   3, 106,\n",
       "        60,   5,  94,  78,  51,  33,  52,  46,  67,  55,  43,  43,  76,\n",
       "        64,  28, 107,  56,  94,  90,  61,  58,  27,  56,  60,  73,  93,\n",
       "        37, 101,  44,  45,  23,  26,  23,  25,  66,   5,  94,  81,  29,\n",
       "        59,  46,   7,  72,  28,   3,  70,  94,  90, 103,  17,  11,  94,\n",
       "        61,  63,  50,  81,   0,  32,  36,  77,  75])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a60f490-4c4f-45b0-9d7b-84049665a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'or noxious, can be built right into homes and neighborhoods. In both cases, the crucial fact is this'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'+!a#CNq8fq\\xc3\\xa0\\xc3\\xa0um\\xe2\\x80\\x93\\xc2\\xa3\\nwF{zxZAAAt*&W&1\\xe2\\x80\\x93\\xce\\xbbP{E!\\xe2\\x80\\x9d]#\\xc2\\xb7pSATNeWKKnb:\\xe2\\x80\\xa2X\\xc2\\xb7}^Z9X]k\\xc2\\xb1E\\xe2\\x80\\x93LM5857d#\\xc2\\xb7s;[N%j:!h\\xc2\\xb7}\\xe2\\x80\\x98/)\\xc2\\xb7^aRs[UNK]?Dom'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ca608-f735-46b3-be33-ebc82b12c4a1",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "## 2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "204b8bb7-2425-44c4-b18a-fb17f436950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e85c2468-0837-4961-843f-1c98eca2cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 108)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.6808763, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b463538-5c37-4747-8e09-73edabae72d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.86455"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b77569c-ecde-4c45-971b-d1915b3e5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "886c783c-6f50-4763-8281-4405bec532c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a79091bf-aaa4-4285-9594-a7b628d710b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2f92417-e928-4e81-af0f-e806e0e1c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 1.0756\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 1.0338\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.9946\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.9576\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.9204\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.8829\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 5s 13ms/step - loss: 0.8459\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.8093\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.7721\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.7361\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 4s 11ms/step - loss: 0.7009\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 4s 11ms/step - loss: 0.6692\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.6415\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.6154\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.5951\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.5750\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.5584\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 4s 11ms/step - loss: 0.5469\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.5363\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 4s 12ms/step - loss: 0.5291\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a10972-bd7d-4e8b-b8c3-a79fbd7642ee",
   "metadata": {},
   "source": [
    "<font color=\"#6A0DAD\">\n",
    "\n",
    "## 3 generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acb03cbe-e4a5-4c31-8756-20ce34687120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2856835d-00da-4c42-ada8-f7d5f12d2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e6a8077-08fb-48b4-ac08-9c028b30a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Pattern Language is the wesce based on his party grodvided the system rent in touch with the overall phape of these situations.\n",
      "Now of hervowed contact with the kind of level parts of rentant and computer swimming political and human computs governing fingers — while would be possible to hill understand their shoulders and to tend to the family room in the following number of stress hinger. And it is not only place in a way which in a serious difficult for metapous there for this physical house — when the ground shell sollow the continuity.\n",
      "... at each activities or not, to find himself and the fill the slab will be last high rise few ways of developing, the reasoning for the kitchen to the common land which it need.\n",
      "Since the thickness of the community in place, these places are not learnd it and means to be traffic. In addition, this seems bottong to the following sequence of people all around the outskilm, semorg — When room for a day; 12 feet deep, 243 EFFICIENT STRUCTURE (206).\n",
      "3 The pattern to the building as a position which touches at that potential to build up groups shave shelter. There is no problem, this pattern gives the Autonomy and Meaningrim different subcultures.” The Indian pile shake was student on no account like up to the pries, contact with nothing like this feeling of arrangement of this drawing.” This is an attect the control of some 50 per cent enclosute to these problems does freedom to create a network of learning.\n",
      "Where bikes are reasons for a computerizated according to this sequence into the full version of the full mean number of a children’s room — see his paper, “On Sitterner City Study of Edvarting Nobod, Diagon Without INDOOR SPACE (191)....\n",
      "182 BUILDING FRONTS*\n",
      "... this pattern gives the form of cloths can be sure that the greenboursen is critical and whether they go to waste in the sufe-stone exception; and the setting diameter of the situation households for fitterling a single simple place.\n",
      "Therefore:\n",
      "Instead of the descriptions and the possibilities for example. The morning will be, the less people are afraid to go out of the realm of behavior and most often doors can be.\n",
      "18 The shape of a house for yourself how either conceptual, control then, is that thing is some, it needs to be part of it apart.\n",
      "The significant gap on them or tile, do not littbat help.\n",
      "Luge freedomed minds which provides no roof of parts can be deeled in positions and so on. When possible, indicate the pattern to be desired a family room. There is the great Ither outdoor room at the same time, to build the evolutional results of the floor of a proximity campant nonw. The other schools — the urban collection can be given exactment to the importance of the kitchen. But the working groups are simply not the kitchen, so does so important and emotionally confirm and sun.; find the philopopies of a building may look need and understood while they are the only the space between the two.\n",
      "Therefore;\n",
      "Build the foundations them out over the terrace, and the flowers  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 7.7744481563568115\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['A Pattern Language is'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(3000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
